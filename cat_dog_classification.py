# -*- coding: utf-8 -*-
"""cat_dog_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V21DvoviRLpRsauRjfaqFhbzisgV4bq2

# **Cats Vs Dogs Classification using Convolution Neural Network by Sakshi Srivastava**

**Problem Statement:**

The task at hand involves classifying images of cats and dogs using a Convolutional Neural Network (CNN).

We have a dataset containing images of cats and dogs and the objective is to train a CNN model to accurately predict the class of each image.

Objectives:

Data Preparation:

● Download and extract the cats vs. dogs dataset.

● Organize the dataset into a structured directory format suitable for TensorFlow’s ImageDataGenerator.

● Split the dataset into training and testing sets.

Data Augmentation and Preprocessing:

● Implement data augmentation techniques to increase the diversity of the training dataset, aiding in the model's ability to generalize.

● Normalize the pixel values of the images for optimal model performance. Model Building:

● Construct a Convolutional Neural Network using TensorFlow and Keras.

● The model should contain multiple convolutional layers, pooling layers, and fully connected layers.

Training:

● Compile the model and train it on the prepared dataset.

● Utilize categorical cross entropy as the loss function and stochastic gradient descent as the optimizer.

● Train the model for a sufficient number of epochs to achieve good performance.

Evaluation:

● Evaluate the model's performance on the validation set during training to monitor for overfitting.

● After training, assess the model's accuracy and make predictions on the test set.

Prediction:

● Implement a system to make predictions on new images, categorizing them as either cat or dog

The system should be able to take an image (or a batch of images), preprocess it, and pass it through the model for predictions.

**LOADING DATASET FROM KAGGLE**

Dataset URL: https://www.kaggle.com/datasets/salader/dogs-vs-cats
"""

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle          # Create hidden kaggle folder
!cp kaggle.json ~/.kaggle/   # Move your json file
!chmod 600 ~/.kaggle/kaggle.json  # Secure it

!kaggle datasets download -d salader/dogs-vs-cats

"""**Loading Files from Zip**"""

import zipfile
zip_ref=zipfile.ZipFile('/content/dogs-vs-cats.zip','r')
zip_ref.extractall('/content')
zip_ref.close()

"""**IMPORTING LIBRARIES**"""

import matplotlib.pyplot as plt
import tensorflow as tf
import pandas as pd
import numpy as np

import warnings
warnings.filterwarnings('ignore')
import tensorflow as tf
from tensorflow import keras
from keras import Sequential
from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,BatchNormalization,Dropout
from tensorflow.keras.utils import image_dataset_from_directory
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img
from tensorflow.keras.preprocessing import image_dataset_from_directory

import os
import matplotlib.image as mpimg

"""**Visualization of training and testing directory**"""

fig = plt.gcf()
fig.set_size_inches(10,10)

cat_dir = os.path.join('/content/train/cats')
dog_dir = os.path.join('/content/train/dogs')
cat_names = os.listdir(cat_dir)
dog_names = os.listdir(dog_dir)

pic_index = 210
cat_images = [os.path.join(cat_dir,fname) for fname in cat_names[pic_index-8:pic_index]]
dog_images = [os.path.join(dog_dir,fname) for fname in dog_names[pic_index-8:pic_index]]

for i,img_path in enumerate(cat_images + dog_images):
     sp = plt.subplot(4,4,i+1)
     sp.axis('off')
     img = mpimg.imread(img_path)
     plt.imshow(img)
     plt.title(f"Label : {img_path[19:22]}")

plt.tight_layout()
plt.show()

#Visualization of test_directory
fig = plt.gcf()
fig.set_size_inches(10,10)

cat_dir = os.path.join('/content/test/cats')
dog_dir = os.path.join('/content/test/dogs')
cat_names = os.listdir(cat_dir)
dog_names = os.listdir(dog_dir)

pic_index = 210
cat_images = [os.path.join(cat_dir,fname) for fname in cat_names[pic_index-8:pic_index]]
dog_images = [os.path.join(dog_dir,fname) for fname in dog_names[pic_index-8:pic_index]]

for i,img_path in enumerate(cat_images + dog_images):
     sp = plt.subplot(4,4,i+1)
     sp.axis('off')
     img = mpimg.imread(img_path)
     plt.imshow(img)
     plt.title(f"Label : {img_path[18:21]}")

plt.tight_layout()
plt.show()

#generators-batches made from the dataset
train_ds=keras.utils.image_dataset_from_directory(
    directory='/content/train',
    labels="inferred",
    label_mode="int",  #cat 0 dog 1 will be assigned
    color_mode="rgb",
    batch_size=32,
    image_size=(256, 256), #same size
)

#generators-batches made from the dataset
validation_ds=keras.utils.image_dataset_from_directory(
    directory='/content/test',
    labels="inferred",
    label_mode="int",  #cat 0 dog 1 will be assigned
    color_mode="rgb",
    batch_size=32,
    image_size=(256, 256), #same size
)

"""images are stores as numpy array,every value is from 0to 255 so we need to normalize it from 0 to 1(pixel)"""

#Normalize
def normalise(image,label):
  image=tf.cast(image/255. ,tf.float32)
  return image,label
train_ds=train_ds.map(normalise)
validation_ds=validation_ds.map(normalise)

"""**MODEL BUILDING**"""

#CNN
model=Sequential()
model.add(Conv2D(32,kernel_size=(3,3),padding='valid',activation='relu',input_shape=(256,256,3)))
model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))
model.add(Conv2D(64,kernel_size=(3,3),padding='valid',activation='relu',input_shape=(256,256,3)))
model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))
model.add(Conv2D(128,kernel_size=(3,3),padding='valid',activation='relu',input_shape=(256,256,3)))
model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))

model.add(Flatten())

model.add(Dense(128,activation='relu'))
model.add(Dense(64,activation='relu'))
model.add(Dense(1,activation='sigmoid'))

model.summary()

"""**COMPILING MODEL**"""

model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

history=model.fit(train_ds,epochs=10,validation_data=validation_ds)

# Evalaute the model

loss, acc = model.evaluate(train_ds)
print(f"\nAccuracy of train: {acc:.4f}\n")

losst, acct = model.evaluate(validation_ds)
print(f"\nAccuracy of test : {acct:.4f}")

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'],color='green',label='train')
plt.plot(history.history['val_accuracy'],color='red',label='validation')
plt.legend()
plt.show()

"""epoch increasing, training accuracy is improving but validation  accuracy is almost 75 to 80%

the gap between training accuracy and validation  accuracy is representing overfitting
"""

plt.plot(history.history['loss'],color='green',label='train')
plt.plot(history.history['val_loss'],color='red',label='validation')
plt.legend()
plt.show()

"""epoch increasing,training loss decreasing whereas validation loss is increasing,hence gap increasing which leads to overfittng...
to avoid overfitting
* add more data
* data augmentation
* dropout
* l1/l2 regularisation
* batch norm

**MODEL WITH IMPROVEMENT**
"""

#CNN + batch normalization and dropout
model=Sequential()
model.add(Conv2D(32,kernel_size=(3,3),padding='valid',activation='relu',input_shape=(256,256,3)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))

model.add(Conv2D(64,kernel_size=(3,3),padding='valid',activation='relu',input_shape=(256,256,3)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))

model.add(Conv2D(128,kernel_size=(3,3),padding='valid',activation='relu',input_shape=(256,256,3)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))

model.add(Flatten())

model.add(Dense(128,activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(64,activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(1,activation='sigmoid'))

model.summary()

model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

history=model.fit(train_ds,epochs=10,validation_data=validation_ds)

#Saving model
model.save('cats_vs_dogs_cnn_model.keras')

# Evaluvate
loss,acc = model.evaluate(train_ds, batch_size = 32, verbose = 0)

print('The accuracy of the model for training data is:',acc*100)
print('The Loss of the model for training data is:',loss)

# Evaluvate
loss,acc = model.evaluate(validation_ds, batch_size = 32, verbose = 0)

print('The accuracy of the model for testing data is:',acc*100)
print('The Loss of the model for testing data is:',loss)

# Evalaute the model

loss, acc = model.evaluate(train_ds)
print(f"\nAccuracy of Train: {acc:.4f}\n")

losst, acct = model.evaluate(validation_ds)
print(f"\nAccuracy of validation : {acct:.4f}")

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'],color='green',label='train')
plt.plot(history.history['val_accuracy'],color='red',label='validation')
plt.legend()
plt.show()

plt.plot(history.history['loss'],color='green',label='train')
plt.plot(history.history['val_loss'],color='red',label='validation')
plt.legend()
plt.show()

"""**PREDICTION**"""

import cv2

test_img=cv2.imread('/content/test/dogs/dog.100.jpg')

plt.imshow(test_img)

test_img.shape

test_img=cv2.resize(test_img,(256,256))

plt.imshow(test_img)

test_input=test_img.reshape((1,256,256,3))

model.predict(test_input)

import cv2
test_img=cv2.imread('/content/test/cats/cat.10007.jpg')
plt.imshow(test_img)

test_img=cv2.resize(test_img,(256,256))
plt.imshow(test_img)

test_input=test_img.reshape((1,256,256,3))
model.predict(test_input)

